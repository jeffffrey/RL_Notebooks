{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Basic Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Monte Carlo ?\n",
    "\n",
    "We assume we know the transition probability of all state. However, we never actually play the game. In Monte Carlo, we learn from experience. We play a few episode, and calculate the average return. \n",
    "\n",
    "We don't need to visit all states. \n",
    "\n",
    "We use explore start technique to make sure we have adequet amount of access to different states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Grid:\n",
    "    def __init__(self, width, height, start):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "        \n",
    "    def set(self, rewards, actions):\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "        \n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "        \n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "    \n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "    \n",
    "    def move(self, action):\n",
    "        \n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "            \n",
    "        return self.rewards.get((self.i, self.j), 0)\n",
    "    \n",
    "    def undo_move(self, action):\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "            \n",
    "        \n",
    "        assert(self.current_state() in self.all_states())\n",
    "        \n",
    "    \n",
    "    def game_over(self):\n",
    "        \n",
    "        return (self.i, self.j) not in self.actions\n",
    "        \n",
    "    def all_states(self):\n",
    "        \n",
    "        return set(list(self.actions.keys()) + list(self.rewards.keys()))\n",
    "    \n",
    "    \n",
    "    def standard_grid():\n",
    "        \n",
    "        g = Grid(3,4,(2,0))\n",
    "        rewards = {(0,3):1, (1,3): -1}\n",
    "        actions = {\n",
    "            (0, 0): ('D', 'R'),\n",
    "            (0, 1): ('L', 'R'),\n",
    "            (0, 2): ('L', 'D', 'R'),\n",
    "            (1, 0): ('U', 'D'),\n",
    "            (1, 2): ('U', 'D', 'R'),\n",
    "            (2, 0): ('U', 'R'),\n",
    "            (2, 1): ('L', 'R'),\n",
    "            (2, 2): ('L', 'R', 'U'),\n",
    "            (2, 3): ('L', 'U'),\n",
    "        }\n",
    "        \n",
    "        g.set(rewards, actions)\n",
    "        return g\n",
    "    \n",
    "    def negative_grid(step_cost=-0.1):\n",
    "        g = Grid.standard_grid()\n",
    "        g.rewards.update({\n",
    "            (0, 0): step_cost,\n",
    "            (0, 1): step_cost,\n",
    "            (0, 2): step_cost,\n",
    "            (1, 0): step_cost,\n",
    "            (1, 2): step_cost,\n",
    "            (2, 0): step_cost,\n",
    "            (2, 1): step_cost,\n",
    "            (2, 2): step_cost,\n",
    "            (2, 3): step_cost,\n",
    "            \n",
    "        })\n",
    "        \n",
    "        return g\n",
    "\n",
    "\n",
    "def print_values(V, g):\n",
    "    \n",
    "    print(\"width:%d, height:%d\" % (g.width, g.height))\n",
    "    \n",
    "    for i in range(g.width):\n",
    "        print(\"------------------------\")\n",
    "        \n",
    "        for j in range(g.height):\n",
    "            v = V.get((i,j), 0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\")\n",
    "        \n",
    "        print (\"\")\n",
    "        \n",
    "def print_policy(P, g):\n",
    "    for i in range(g.width):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.height):\n",
    "            a = P.get((i,j), 0)\n",
    "            print(\" %s |\" % a, end=\"\")\n",
    "        print (\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "width:3, height:4\n",
      "------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "Episode 0\n",
      "Episode 1\n",
      "Episode 2\n",
      "Episode 3\n",
      "Episode 4\n",
      "Episode 5\n",
      "Episode 6\n",
      "Episode 7\n",
      "Episode 8\n",
      "Episode 9\n",
      "Episode 10\n",
      "Episode 11\n",
      "Episode 12\n",
      "Episode 13\n",
      "Episode 14\n",
      "Episode 15\n",
      "Episode 16\n",
      "Episode 17\n",
      "Episode 18\n",
      "Episode 19\n",
      "Episode 20\n",
      "Episode 21\n",
      "Episode 22\n",
      "Episode 23\n",
      "Episode 24\n",
      "Episode 25\n",
      "Episode 26\n",
      "Episode 27\n",
      "Episode 28\n",
      "Episode 29\n",
      "Episode 30\n",
      "Episode 31\n",
      "Episode 32\n",
      "Episode 33\n",
      "Episode 34\n",
      "Episode 35\n",
      "Episode 36\n",
      "Episode 37\n",
      "Episode 38\n",
      "Episode 39\n",
      "Episode 40\n",
      "Episode 41\n",
      "Episode 42\n",
      "Episode 43\n",
      "Episode 44\n",
      "Episode 45\n",
      "Episode 46\n",
      "Episode 47\n",
      "Episode 48\n",
      "Episode 49\n",
      "Episode 50\n",
      "Episode 51\n",
      "Episode 52\n",
      "Episode 53\n",
      "Episode 54\n",
      "Episode 55\n",
      "Episode 56\n",
      "Episode 57\n",
      "Episode 58\n",
      "Episode 59\n",
      "Episode 60\n",
      "Episode 61\n",
      "Episode 62\n",
      "Episode 63\n",
      "Episode 64\n",
      "Episode 65\n",
      "Episode 66\n",
      "Episode 67\n",
      "Episode 68\n",
      "Episode 69\n",
      "Episode 70\n",
      "Episode 71\n",
      "Episode 72\n",
      "Episode 73\n",
      "Episode 74\n",
      "Episode 75\n",
      "Episode 76\n",
      "Episode 77\n",
      "Episode 78\n",
      "Episode 79\n",
      "Episode 80\n",
      "Episode 81\n",
      "Episode 82\n",
      "Episode 83\n",
      "Episode 84\n",
      "Episode 85\n",
      "Episode 86\n",
      "Episode 87\n",
      "Episode 88\n",
      "Episode 89\n",
      "Episode 90\n",
      "Episode 91\n",
      "Episode 92\n",
      "Episode 93\n",
      "Episode 94\n",
      "Episode 95\n",
      "Episode 96\n",
      "Episode 97\n",
      "Episode 98\n",
      "Episode 99\n",
      "final values:\n",
      "width:3, height:4\n",
      "------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.73| 0.00|-1.00| 0.00|\n",
      "------------------------\n",
      " 0.66|-0.81|-0.90|-1.00|\n",
      "final policy:\n",
      "---------------------------\n",
      " R | R | R | 0 |\n",
      "---------------------------\n",
      " U | 0 | R | 0 |\n",
      "---------------------------\n",
      " U | R | R | U |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SMALL_ENOUGH = 1e-3\n",
    "gamma = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "def play_game(grid, policy):\n",
    "    \n",
    "    start_states = list(grid.actions.keys())\n",
    "    start_idx = np.random.choice(len(start_states))\n",
    "    grid.set_state(start_states[start_idx])\n",
    "    \n",
    "    s = grid.current_state()\n",
    "    states_and_rewards = [(s,0)]\n",
    "    \n",
    "    while not grid.game_over():\n",
    "        a = policy[s]\n",
    "        r = grid.move(a)\n",
    "        s = grid.current_state()\n",
    "        states_and_rewards.append((s,r))\n",
    "        \n",
    "    \n",
    "    G = 0\n",
    "    states_and_returns =[]\n",
    "    first = True\n",
    "    \n",
    "    for s, r in reversed(states_and_rewards):\n",
    "        \n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            states_and_returns.append((s,G))\n",
    "            \n",
    "        G = r + gamma * G\n",
    "        \n",
    "    states_and_returns.reverse()\n",
    "    return states_and_returns\n",
    "\n",
    "\n",
    "grid = Grid.standard_grid()\n",
    "\n",
    "print(\"rewards:\")\n",
    "print_values(grid.rewards, grid)\n",
    "\n",
    "\n",
    "# this should be a random action\n",
    "policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U',\n",
    "  }\n",
    "\n",
    "V = {}\n",
    "returns = {}\n",
    "states = grid.all_states()\n",
    "\n",
    "for s in states:\n",
    "    if s in grid.actions:\n",
    "        returns[s] = []\n",
    "    else:\n",
    "        V[s] = 0\n",
    "\n",
    "for t in range(100):\n",
    "    \n",
    "    \n",
    "    print(\"Episode %d\" % t)\n",
    "    states_and_returns = play_game(grid, policy)\n",
    "    seen_states = set()\n",
    "    \n",
    "    for s, G in states_and_returns:\n",
    "        \n",
    "        if s not in seen_states:\n",
    "            returns[s].append(G)\n",
    "            V[s] = np.mean(returns[s])\n",
    "            seen_states.add(s)\n",
    "\n",
    "            \n",
    "            \n",
    "print(\"final values:\")\n",
    "print_values(V, grid)\n",
    "print(\"final policy:\")\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we perform policy optimization. We pick the action that generate best value function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "width:3, height:4\n",
      "------------------------\n",
      "-0.90|-0.90|-0.90| 1.00|\n",
      "------------------------\n",
      "-0.90| 0.00|-0.90|-1.00|\n",
      "------------------------\n",
      "-0.90|-0.90|-0.90|-0.90|\n",
      " iteration : 0\n",
      " iteration : 100\n",
      " iteration : 200\n",
      " iteration : 300\n",
      " iteration : 400\n",
      " iteration : 500\n",
      " iteration : 600\n",
      " iteration : 700\n",
      " iteration : 800\n",
      " iteration : 900\n",
      " iteration : 1000\n",
      " iteration : 1100\n",
      " iteration : 1200\n",
      " iteration : 1300\n",
      " iteration : 1400\n",
      " iteration : 1500\n",
      " iteration : 1600\n",
      " iteration : 1700\n",
      " iteration : 1800\n",
      " iteration : 1900\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f77b405f748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final policy:\n",
      "---------------------------\n",
      " R | R | R | 0 |\n",
      "---------------------------\n",
      " U | 0 | U | 0 |\n",
      "---------------------------\n",
      " R | R | U | U |\n",
      "final values:\n",
      "width:3, height:4\n",
      "------------------------\n",
      "-0.90| 0.00| 1.00| 0.00|\n",
      "------------------------\n",
      "-2.04| 0.00| 0.00| 0.00|\n",
      "------------------------\n",
      "-2.67|-1.99|-1.01|-1.00|\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gamma = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "\n",
    "\n",
    "def play_game(grid, policy):\n",
    "  \n",
    "    start_states = list(grid.actions.keys())\n",
    "    start_idx = np.random.choice(len(start_states))\n",
    "    grid.set_state(start_states[start_idx])\n",
    "\n",
    "    s = grid.current_state()\n",
    "    a = np.random.choice(ALL_POSSIBLE_ACTIONS) # first action is uniformly random\n",
    "    \n",
    "    states_actions_rewards = [(s,a,0)]\n",
    "    seen_states = set()\n",
    "    \n",
    "    while True:\n",
    "        old_s = grid.current_state()\n",
    "        #print(\"old state: %s\" % str(old_s))\n",
    "        r = grid.move(a)\n",
    "        s = grid.current_state()\n",
    "        \n",
    "        if s in seen_states:\n",
    "            \n",
    "            states_actions_rewards.append((s, None, -100))\n",
    "            break\n",
    "        elif grid.game_over():\n",
    "            states_actions_rewards.append((s, None, r))\n",
    "            break\n",
    "        else:\n",
    "            a = policy[s]\n",
    "            states_actions_rewards.append((s,a,r))\n",
    "        seen_states.add(s)\n",
    "\n",
    "        \n",
    "    G = 0\n",
    "    states_actions_returns = []\n",
    "    first = True\n",
    "    for s, a, r in reversed(states_actions_rewards):\n",
    "        \n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            states_actions_returns.append((s, a, G))\n",
    "            \n",
    "        G = r + gamma * G\n",
    "    \n",
    "    states_actions_returns.reverse()\n",
    "    \n",
    "    return states_actions_returns\n",
    "\n",
    "def max_dict(d):\n",
    "    \n",
    "    max_key = None\n",
    "    max_val = float('-inf')\n",
    "    \n",
    "    for k, v in d.items():\n",
    "        \n",
    "        if v > max_val:\n",
    "            max_val = v\n",
    "            max_key = k\n",
    "            \n",
    "    return max_key, max_val\n",
    "        \n",
    "\n",
    "grid = Grid.negative_grid(step_cost=-0.9)\n",
    "\n",
    "print(\"rewards:\")\n",
    "print_values(grid.rewards, grid)\n",
    "\n",
    "\n",
    "policy = {}\n",
    "\n",
    "for s in list(grid.actions.keys()):\n",
    "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "\n",
    "Q = {}\n",
    "returns = {}\n",
    "states = grid.all_states()\n",
    "\n",
    "for s in states:\n",
    "    if s in grid.actions:\n",
    "        Q[s] = {}\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "            Q[s][a] = 0\n",
    "            returns[(s,a)] = []\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "deltas = []\n",
    "\n",
    "for t in range(2000):\n",
    "    if t % 100 == 0:\n",
    "        print(\" iteration : %d\" % t)\n",
    "        \n",
    "    biggest_change = 0\n",
    "    states_actions_returns = play_game(grid, policy)\n",
    "    seen_state_action_pairs = set()\n",
    "    \n",
    "    for s, a, G in states_actions_returns:\n",
    "        \n",
    "        sa = (s, a)\n",
    "        if sa not in seen_state_action_pairs:\n",
    "            old_q = Q[s][a]\n",
    "            returns[sa].append(G)\n",
    "            Q[s][a] = np.mean(returns[sa])\n",
    "            \n",
    "            biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
    "            seen_state_action_pairs.add(sa)\n",
    "        \n",
    "        deltas.append(biggest_change)\n",
    "        \n",
    "        for s in policy.keys():\n",
    "            policy[s] = max_dict(Q[s])[0]\n",
    "            \n",
    "plt.plot(deltas)\n",
    "plt.show()\n",
    "          \n",
    "    \n",
    "\n",
    "print(\"final policy:\")\n",
    "print_policy(policy, grid)\n",
    "\n",
    "  # find V\n",
    "V = {}\n",
    "for s, Qs in Q.items():\n",
    "    V[s] = max_dict(Q[s])[1]\n",
    "\n",
    "print(\"final values:\")\n",
    "print_values(V, grid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
